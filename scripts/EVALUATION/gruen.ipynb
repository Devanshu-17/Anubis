{"cells":[{"cell_type":"markdown","metadata":{"id":"ZPLp4DpB9nFo"},"source":["# SETUP"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T11:11:55.106271Z","iopub.status.busy":"2024-06-13T11:11:55.105624Z","iopub.status.idle":"2024-06-13T11:14:41.237519Z","shell.execute_reply":"2024-06-13T11:14:41.236207Z","shell.execute_reply.started":"2024-06-13T11:11:55.106227Z"},"id":"5wPqUDCjv9SZ","outputId":"5807bb1d-0f12-4c2a-996f-c37fcdb47cc1","trusted":true},"outputs":[],"source":["!git clone https://github.com/WanzhengZhu/GRUEN.git\n","!pip install -r /kaggle/working/GRUEN/requirements.txt\n","\n","!chmod u+x /kaggle/working/GRUEN/install.sh & /kaggle/working/GRUEN/install.sh\n","\n","!pip install gdown\n","\n","import gdown\n","import zipfile\n","import os\n","\n","# Define the URL and output file name\n","url = \"https://drive.google.com/uc?id=1Hw5na_Iy4-kGEoX60bD8vXYeJDQrzyj6\"\n","output_zip = \"/kaggle/working/cola_model.zip\"\n","output_dir = \"/kaggle/working/\"\n","\n","# Download the file\n","gdown.download(url, output_zip, quiet=False)\n","\n","# Unzip the file\n","with zipfile.ZipFile(output_zip, 'r') as zip_ref:\n","    zip_ref.extractall(output_dir)\n","\n","print(f\"Downloaded and extracted {output_zip} to {output_dir}\")\n","\n","# Clean up the zip file\n","os.remove(output_zip)\n","print(f\"Removed the zip file: {output_zip}\")\n","\n","!pip install wmd\n","\n","import nltk\n","nltk.download('punkt')\n","\n","!pip install editdistance"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T11:14:41.241131Z","iopub.status.busy":"2024-06-13T11:14:41.239819Z","iopub.status.idle":"2024-06-13T11:14:58.007192Z","shell.execute_reply":"2024-06-13T11:14:58.006153Z","shell.execute_reply.started":"2024-06-13T11:14:41.241091Z"},"id":"Y816Qb5fwyYY","trusted":true},"outputs":[],"source":["import difflib\n","import pandas as pd\n","import editdistance\n","import math\n","import numpy as np\n","import re\n","import spacy\n","from spacy.language import Language\n","import string\n","import torch\n","from nltk.tokenize import sent_tokenize\n","from tqdm import tqdm\n","from transformers import BertConfig, BertForSequenceClassification, BertTokenizer, BertForMaskedLM\n","from transformers import glue_convert_examples_to_features, logging\n","from transformers.data.processors.utils import InputExample\n","from wmd import WMD\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\"\"\" Processing \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T11:15:04.733536Z","iopub.status.busy":"2024-06-13T11:15:04.732799Z","iopub.status.idle":"2024-06-13T11:15:04.737909Z","shell.execute_reply":"2024-06-13T11:15:04.736944Z","shell.execute_reply.started":"2024-06-13T11:15:04.733504Z"},"trusted":true},"outputs":[],"source":["\n","\n","saved_pretrained_CoLA_model_dir = '/kaggle/working/cola_model/bert-base-cased/'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T11:15:21.816205Z","iopub.status.busy":"2024-06-13T11:15:21.815839Z","iopub.status.idle":"2024-06-13T11:15:38.735365Z","shell.execute_reply":"2024-06-13T11:15:38.733986Z","shell.execute_reply.started":"2024-06-13T11:15:21.816176Z"},"id":"YJmUJrSewLDv","outputId":"3be18838-dfcf-48be-9d09-fd1c6a840127","trusted":true},"outputs":[],"source":["import os\n","import re\n","import torch\n","import math\n","import numpy as np\n","import pandas as pd\n","import string\n","import spacy\n","from tqdm import tqdm\n","from transformers import BertTokenizer, BertForMaskedLM, BertForSequenceClassification, BertConfig\n","from transformers.data.processors.glue import InputExample, glue_convert_examples_to_features\n","from nltk.tokenize import sent_tokenize\n","from difflib import SequenceMatcher\n","import logging\n","\n","# Directory containing the CSV files\n","csv_dir = '/kaggle/input/dataset-bias/results/'\n","# Directory to save the text files\n","output_dir = '/kaggle/working/'\n","\n","# Check if output directory exists, if not create it\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","# Initialize lists to store GRUEN scores and differences\n","gold_gruen_scores = []\n","new_gruen_scores = []\n","gruen_diffs = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def preprocess_candidates(candidates):\n","    for i in range(len(candidates)):\n","        if pd.isna(candidates[i]):\n","            candidates[i] = \"\"\n","        else:\n","            candidates[i] = str(candidates[i]).strip()\n","            candidates[i] = '. '.join(candidates[i].split('\\n\\n'))\n","            candidates[i] = '. '.join(candidates[i].split('\\n'))\n","            candidates[i] = '.'.join(candidates[i].split('..'))\n","            candidates[i] = '. '.join(candidates[i].split('.'))\n","            candidates[i] = '. '.join(candidates[i].split('. . '))\n","            candidates[i] = '. '.join(candidates[i].split('.  . '))\n","            while len(candidates[i].split('  ')) > 1:\n","                candidates[i] = ' '.join(candidates[i].split('  '))\n","            myre = re.search(r'(\\d+)\\. (\\d+)', candidates[i])\n","            while myre:\n","                candidates[i] = 'UNK'.join(candidates[i].split(myre.group()))\n","                myre = re.search(r'(\\d+)\\. (\\d+)', candidates[i])\n","            candidates[i] = candidates[i].strip()\n","    processed_candidates = []\n","    for candidate_i in candidates:\n","        sentences = sent_tokenize(candidate_i)\n","        out_i = []\n","        for sentence_i in sentences:\n","            if len(sentence_i.translate(str.maketrans('', '', string.punctuation)).split()) > 1:  # More than one word.\n","                out_i.append(sentence_i)\n","        processed_candidates.append(out_i)\n","    return processed_candidates\n","\n","def get_lm_score(sentences):\n","    def score_sentence(sentence, tokenizer, model):\n","        tokenize_input = tokenizer.tokenize(sentence)\n","        if len(tokenize_input) > 510:\n","            tokenize_input = tokenize_input[:510]\n","        input_ids = torch.tensor(tokenizer.encode(tokenize_input)).unsqueeze(0).to(device)\n","        with torch.no_grad():\n","            loss = model(input_ids, labels=input_ids)[0]\n","        return math.exp(loss.item())\n","\n","    model_name = 'bert-base-cased'\n","    model = BertForMaskedLM.from_pretrained(model_name).to(device)\n","    model.eval()\n","    tokenizer = BertTokenizer.from_pretrained(model_name)\n","    lm_score = []\n","    for sentence in tqdm(sentences):\n","        if len(sentence) == 0:\n","            lm_score.append(0.0)\n","            continue\n","        score_i = 0.0\n","        for x in sentence:\n","            score_i += score_sentence(x, tokenizer, model)\n","        score_i /= len(sentence)\n","        lm_score.append(score_i)\n","    return lm_score\n","\n","def get_cola_score(sentences):\n","    def load_pretrained_cola_model(model_name, saved_pretrained_CoLA_model_dir):\n","        config_class, model_class, tokenizer_class = (BertConfig, BertForSequenceClassification, BertTokenizer)\n","        config = config_class.from_pretrained(saved_pretrained_CoLA_model_dir, num_labels=2, finetuning_task='CoLA')\n","        tokenizer = tokenizer_class.from_pretrained(saved_pretrained_CoLA_model_dir, do_lower_case=0)\n","        model = model_class.from_pretrained(saved_pretrained_CoLA_model_dir, from_tf=bool('.ckpt' in model_name), config=config).to(device)\n","        model.eval()\n","        return tokenizer, model\n","\n","    def evaluate_cola(model, candidates, tokenizer, model_name):\n","        def load_and_cache_examples(candidates, tokenizer):\n","            max_length = 128\n","            examples = [InputExample(guid=str(i), text_a=x) for i, x in enumerate(candidates)]\n","            features = glue_convert_examples_to_features(\n","                examples, tokenizer, label_list=[\"0\", \"1\"], max_length=max_length, output_mode=\"classification\")\n","            all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","            all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n","            all_labels = torch.tensor([0 for f in features], dtype=torch.long)\n","            all_token_type_ids = torch.tensor([[0.0] * max_length for f in features], dtype=torch.long)\n","            dataset = torch.utils.data.TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n","            return dataset\n","\n","        eval_dataset = load_and_cache_examples(candidates, tokenizer)\n","        eval_dataloader = torch.utils.data.DataLoader(\n","            eval_dataset, sampler=torch.utils.data.SequentialSampler(eval_dataset), batch_size=max(1, torch.cuda.device_count()))\n","        preds = None\n","        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","            model.eval()\n","            batch = tuple(t.to(device) for t in batch)\n","            with torch.no_grad():\n","                inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[3]}\n","                if model_name.split('-')[0] != 'distilbert':\n","                    inputs['token_type_ids'] = batch[2] if model_name.split('-')[0] in ['bert', 'xlnet'] else None\n","                outputs = model(**inputs)\n","                tmp_eval_loss, logits = outputs[:2]\n","            if preds is None:\n","                preds = logits.detach().cpu().numpy()\n","            else:\n","                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n","        return preds[:, 1].tolist()\n","\n","    def convert_sentence_score_to_paragraph_score(sentence_score, sent_length):\n","        paragraph_score = []\n","        pointer = 0\n","        for i in sent_length:\n","            if i == 0:\n","                paragraph_score.append(0.0)\n","                continue\n","            temp_a = sentence_score[pointer:pointer + i]\n","            paragraph_score.append(sum(temp_a) / len(temp_a))\n","            pointer += i\n","        return paragraph_score\n","\n","    model_name = 'bert-base-cased'\n","    saved_pretrained_CoLA_model_dir = './cola_model/' + model_name + '/'\n","    tokenizer, model = load_pretrained_cola_model(model_name, saved_pretrained_CoLA_model_dir)\n","    candidates = [y for x in sentences for y in x]\n","    sent_length = [len(x) for x in sentences]\n","    cola_score = evaluate_cola(model, candidates, tokenizer, model_name)\n","    cola_score = convert_sentence_score_to_paragraph_score(cola_score, sent_length)\n","    return cola_score\n","\n","def get_grammaticality_score(processed_candidates):\n","    lm_score = get_lm_score(processed_candidates)\n","    cola_score = get_cola_score(processed_candidates)\n","    grammaticality_score = [1.0 * math.exp(-0.5 * x) + 1.0 * y for x, y in zip(lm_score, cola_score)]\n","    grammaticality_score = [max(0, x / 8.0 + 0.5) for x in grammaticality_score]  # re-scale\n","    return grammaticality_score\n","\n","def get_redundancy_score(all_summary):\n","    def if_two_sentence_redundant(a, b):\n","        if a == b:\n","            return 4\n","        if (a in b) or (b in a):\n","            return 4\n","        flag_num = 0\n","        a_split = a.split()\n","        b_split = b.split()\n","        if max(len(a_split), len(b_split)) >= 5:\n","            longest_common_substring = SequenceMatcher(None, a, b).find_longest_match(0, len(a), 0, len(b))\n","            LCS_string_length = longest_common_substring.size\n","            if LCS_string_length > 0.8 * min(len(a), len(b)):\n","                flag_num += 1\n","            LCS_word_length = len(a[longest_common_substring[0]:(longest_common_substring[0] + LCS_string_length)].strip().split())\n","            if LCS_word_length > 0.8 * min(len(a_split), len(b_split)):\n","                flag_num += 1\n","            edit_distance = editdistance.eval(a, b)\n","            if edit_distance < 0.6 * max(len(a), len(b)):  # Number of modifications from the longer sentence is too small.\n","                flag_num += 1\n","            number_of_common_word = len([x for x in a_split if x in b_split])\n","            if number_of_common_word > 0.8 * min(len(a_split), len(b_split)):\n","                flag_num += 1\n","        return flag_num\n","\n","    redundancy_score = [0.0 for x in range(len(all_summary))]\n","    for i in range(len(all_summary)):\n","        flag = 0\n","        summary = all_summary[i]\n","        if len(summary) == 1:\n","            continue\n","        for j in range(len(summary) - 1):  # for pairwise redundancy\n","            for k in range(j + 1, len(summary)):\n","                flag += if_two_sentence_redundant(summary[j].strip(), summary[k].strip())\n","        redundancy_score[i] += -0.1 * flag\n","    return redundancy_score\n","\n","@Language.component(\"simhook\")\n","def SimilarityHook(doc):\n","    nlp = spacy.load('en_core_web_md')\n","    nlp.add_pipe('simhook', last=True)\n","    return doc\n","\n","def get_focus_score(all_summary):\n","    def compute_sentence_similarity():\n","        nlp = spacy.load('en_core_web_md')\n","        nlp.add_pipe('simhook', last=True)\n","        all_score = []\n","        for i in range(len(all_summary)):\n","            if len(all_summary[i]) == 1:\n","                all_score.append([1.0])\n","                continue\n","            score = []\n","            for j in range(1, len(all_summary[i])):\n","                doc1 = nlp(all_summary[i][j - 1])\n","                doc2 = nlp(all_summary[i][j])\n","                try:\n","                    score.append(1.0 / (1.0 + math.exp(-doc1.similarity(doc2) + 7)))\n","                except:\n","                    score.append(1.0)\n","            all_score.append(score)\n","        return all_score\n","\n","    all_score = compute_sentence_similarity()\n","    focus_score = [0.0 for x in range(len(all_summary))]\n","    for i in range(len(all_score)):\n","        if len(all_score[i]) == 0:\n","            continue\n","        if min(all_score[i]) < 0.05:\n","            focus_score[i] -= 0.1\n","    return focus_score\n","\n","def get_gruen(candidates):\n","    processed_candidates = preprocess_candidates(candidates)\n","    grammaticality_score = get_grammaticality_score(processed_candidates)\n","    redundancy_score = get_redundancy_score(processed_candidates)\n","    focus_score = get_focus_score(processed_candidates)\n","    # coherence_score = get_coherence_score(processed_candidates)\n","    # We do not release the code for the coherence score calculation for this version.\n","    # We are working on a more efficient and reliable approach now and will release it later.\n","    gruen_score = [min(1, max(0, sum(i))) for i in zip(grammaticality_score, redundancy_score, focus_score)]\n","    return gruen_score\n","\n","# Iterate over all CSV files in the directory\n","for csv_file in os.listdir(csv_dir):\n","    if csv_file.endswith('.csv'):\n","        file_path = os.path.join(csv_dir, csv_file)\n","        df = pd.read_csv(file_path)\n","\n","        # Initialize lists to store GRUEN scores and differences for each file\n","        gold_gruen_scores = []\n","        new_gruen_scores = []\n","        gruen_diffs = []\n","\n","        # Iterate over the rows in the dataset\n","        for index, row in df.iterrows():\n","            gold_sentence = row['Debiased Sentence']\n","            new_sentence = row['LLM Debiased Sentence']\n","\n","            # Prepare the text list for PPL calculation\n","            text = [gold_sentence, new_sentence]\n","\n","            # Calculate GRUEN scores\n","            candidates = [gold_sentence, new_sentence]\n","            gruen_scores = get_gruen(candidates)\n","\n","            # Store the GRUEN scores, replace NaN with 0\n","            gold_gruen_scores.append(gruen_scores[0] if not np.isnan(gruen_scores[0]) else 0)\n","            new_gruen_scores.append(gruen_scores[1] if not np.isnan(gruen_scores[1]) else 0)\n","            gruen_diffs.append((gruen_scores[1] if not np.isnan(gruen_scores[1]) else 0) - (gruen_scores[0] if not np.isnan(gruen_scores[0]) else 0))\n","\n","        # Add the new columns to the dataframe\n","        df['gold_gruen_score'] = gold_gruen_scores\n","        df['new_gruen_score'] = new_gruen_scores\n","        df['gruen_diff'] = gruen_diffs\n","\n","        # Calculate the averages\n","        avg_gold_gruen = sum(gold_gruen_scores) / len(gold_gruen_scores)\n","        avg_new_gruen = sum(new_gruen_scores) / len(new_gruen_scores)\n","        avg_gruen_diff = sum(gruen_diffs) / len(gruen_diffs)\n","\n","        print(f\"File: {csv_file}\")\n","        print(f\"Average gold GRUEN score: {avg_gold_gruen}\")\n","        print(f\"Average new GRUEN score: {avg_new_gruen}\")\n","        print(f\"Average GRUEN difference: {avg_gruen_diff}\")\n","\n","        # Save the results to a text file\n","        results_text = (\n","            f\"File: {csv_file}\\n\"\n","            f\"Average gold GRUEN score: {avg_gold_gruen}\\n\"\n","            f\"Average new GRUEN score: {avg_new_gruen}\\n\"\n","            f\"Average GRUEN difference: {avg_gruen_diff}\\n\"\n","        )\n","\n","        text_file_path = os.path.join(output_dir, f\"{os.path.splitext(csv_file)[0]}_RESULTS.txt\")\n","\n","        with open(text_file_path, 'w') as text_file:\n","            text_file.write(results_text)\n","\n","        print(f\"Results saved to {text_file_path}\")\n","\n","logging.set_verbosity_error()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5132134,"sourceId":8682203,"sourceType":"datasetVersion"}],"dockerImageVersionId":30716,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
